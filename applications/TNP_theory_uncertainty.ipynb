{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVPvFD6vv-g6"
   },
   "source": [
    "# Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYMvMBY4L1p9"
   },
   "outputs": [],
   "source": [
    "# The user specifies if running notebook on GoogleColab or locally\n",
    "UseGoogleColab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_t5G13ymL1qB"
   },
   "outputs": [],
   "source": [
    "if UseGoogleColab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    USERDIR='/content/drive/MyDrive/hightea/'\n",
    "else:\n",
    "    USERDIR='.'\n",
    "\n",
    "#Install hightea client and plotting library\n",
    "%pip install hightea-client > /dev/null\n",
    "%pip install hightea-plotting > /dev/null\n",
    "from hightea.client.apiactions import API\n",
    "from hightea.plotting import plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XmsBZWAzL1qC"
   },
   "outputs": [],
   "source": [
    "from hightea.client import Interface as hightea\n",
    "from hightea.plotting import Run\n",
    "\n",
    "import numpy as np\n",
    "import random as rn\n",
    "from scipy.interpolate import BPoly\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['text.usetex'] = True\n",
    "mpl.rcParams['text.latex.preamble'] = r'\\usepackage{amsmath}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6LoGw2cvOI2"
   },
   "source": [
    "# Preparations and process definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71IrosyrbjLj"
   },
   "source": [
    "## Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hn4R-vkgYXNW"
   },
   "outputs": [],
   "source": [
    "Nc = 3.\n",
    "# simple aS(mu) interpolation routine based on NNPDF31_nnlo_as_0118.data\n",
    "aS_data=np.array([[10,0.1781227067],[20,0.1534035528],[30,0.1419596493],[40,0.1348485781],[50,0.1298172426],[60,0.1259832582],[70,0.1229184081],[80,0.1203847569],[90,0.1182363831],[100,0.1163807993],\n",
    "                  [110,0.1147523072],[120,0.1133059094],[130,0.1120083271],[140,0.1108332221],[150,0.1097616798],[160,0.1087787423],[170,0.107871636],[180,0.1070300523],[190,0.1062461942],[200,0.1055136367],\n",
    "                  [210,0.1048264566],[220,0.1041796584],[230,0.1035689945],[240,0.1029909189],[250,0.1024427763],[260,0.1019218722],[270,0.1014257855],[280,0.1009523909],[290,0.1004998133],[300,0.1000663915],\n",
    "                  [310,0.09965064739],[320,0.09925144808],[330,0.09886775147],[340,0.09849847318],[350,0.09814263404],[360,0.0977993472],[370,0.09746780716],[380,0.09714728033],[390,0.09683709696],[400,0.09653664405],\n",
    "                  [410,0.09624535934],[420,0.09596277253],[430,0.0956885257],[440,0.09542217809],[450,0.09516331596],[460,0.0949115559],[470,0.09466654201],[480,0.09442794338],[490,0.09419545189],[500,0.09396878017]]).T\n",
    "def get_alphas(mu):\n",
    "    return np.interp(mu, aS_data[0], aS_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dn4iVUOIu62u"
   },
   "outputs": [],
   "source": [
    "processes = {\n",
    "    'pp_evmv_13000':{\n",
    "        'orders':[\"LO\",\"NLO\",\"qqNNLO\"],\n",
    "        'nc_power':1,\n",
    "        'as_power':0,\n",
    "        'label':r'$pp \\to e^+ v_e \\mu^- \\bar{v}_{\\mu}$ LHC @ 13 TeV',\n",
    "        'observables':{\n",
    "\n",
    "            'mww':{\n",
    "                'definition':'sqrt((p_lp_0+p_lm_0+p_vp_0+p_vm_0)**2 - (p_lp_1+p_lm_1+p_vp_1+p_vm_1)**2 - (p_lp_2+p_lm_2+p_vp_2+p_vm_2)**2 - (p_lp_3+p_lm_3+p_vp_3+p_vm_3)**2)',\n",
    "                'label':r'$m(W^+W^-)$ [GeV]',\n",
    "                'binning':list(np.logspace(np.log10(160.),np.log10(500.),10)),\n",
    "            },\n",
    "            'ptlm':{\n",
    "                'definition':'sqrt((p_lm_1)**2 + (p_lm_2)**2)',\n",
    "                'label':r'$p_T(\\mu^-)$ [GeV]',\n",
    "                'binning':list([20.,50.,100.,200.,300.]),\n",
    "            },\n",
    "            'ywm':{\n",
    "                'definition':'0.5*log((p_lm_0+p_vm_0 + p_lm_3+p_vm_3)/(p_lm_0+p_vm_0 - p_lm_3-p_vm_3))',\n",
    "                'label':r'$y(W^-)$',\n",
    "                'binning':list(np.linspace(-3.,3.,7)),\n",
    "            },\n",
    "        },\n",
    "        'scales':{\n",
    "            'HTh':{\n",
    "                'definition':'HT/2.',\n",
    "                'label':r'$H_T/2$',\n",
    "            },\n",
    "            'mw':{\n",
    "                'definition':'80.3790+ywm*0.', # hack to allow to differentially bin a constant number\n",
    "                'label':r'$m_W$',\n",
    "            }\n",
    "        },\n",
    "        'jobs':{\n",
    "        }\n",
    "    },\n",
    "     'pp_ww_13000':{\n",
    "         'orders':[\"LO\",\"NLO\",\"NNLO\"],\n",
    "         'nc_power':1,\n",
    "         'as_power':0,\n",
    "         'label':r'$pp \\to W^+ W^-$ LHC @ 13 TeV',\n",
    "         'observables':{\n",
    "             'mww':{\n",
    "                 'definition':'sqrt((p_wp_0+p_wm_0)**2 - (p_wp_1+p_wm_1)**2 - (p_wp_2+p_wm_2)**2 - (p_wp_3+p_wm_3)**2)',\n",
    "                 'label':r'$m(W^+W^-)$ [GeV]',\n",
    "                 'binning':list(np.logspace(np.log10(160.),np.log10(500.),10)),\n",
    "              },\n",
    "              'ptwp':{\n",
    "                  'definition':'sqrt(p_wp_1**2 + p_wp_2**2)',\n",
    "                  'label':r'$p_T(W^+)$ [GeV]',\n",
    "                  'binning':list([0.,100,200,300,400,500.]),\n",
    "              },\n",
    "              'yww':{\n",
    "                 'definition':'0.5*log((p_wp_0+p_wm_0 + p_wp_3+p_wm_3)/(p_wp_0 - p_wp_3+p_wm_0 - p_wm_3))',\n",
    "                 'label':r'$y(W^+W^-)$',\n",
    "                 'binning':list(np.linspace(-3.,3.,11)),\n",
    "             },\n",
    "          },\n",
    "          'scales':{\n",
    "              'HTh':{\n",
    "                  'definition':'(sqrt(80.3790**2 + pt_wp**2)+sqrt(80.3790**2+pt_wm**2))/2.',\n",
    "                  'label':r'$H_T/2$',\n",
    "              },\n",
    "             'mw':{\n",
    "                 'definition':'80.3790+mww*0.', # hack to allow to differentially bin a constant number\n",
    "                 'label':r'$m_W$',\n",
    "             }\n",
    "         },\n",
    "         'jobs':{\n",
    "         }\n",
    "    },\n",
    "    'pp_tt_13000_172.5':{\n",
    "        'orders':[\"LO\",\"NLO\",\"NNLO\"],\n",
    "        'nc_power':4,\n",
    "        'as_power':2,\n",
    "        'label':r'$pp \\to t \\bar{t}$ LHC @ 13 TeV',\n",
    "        'observables':{\n",
    "            'mtt':{\n",
    "                'definition':'sqrt((p_t_0+p_tbar_0)**2-(p_t_1+p_tbar_1)**2-(p_t_2+p_tbar_2)**2-(p_t_3+p_tbar_3)**2)',\n",
    "                'label':r'$m(t\\bar{t})$ [GeV]',\n",
    "                'binning':list([345.,390,440.,480.,530.,570.,620.,665.,710.,755.,800.]),\n",
    "            },\n",
    "            'pTt':{\n",
    "                'definition':'sqrt(p_t_1**2 + p_t_2**2)',\n",
    "                'label':r'$p_T(t)$ [GeV]',\n",
    "                'binning':list([0.,100.,200.,300.,400.,800.]),\n",
    "            },\n",
    "            'yt':{\n",
    "                'definition':'0.5*log((p_t_0 + p_t_3)/(p_t_0 - p_t_3))',\n",
    "                'label':r'$y(t)$',\n",
    "                'binning':list(np.linspace(-3.,3.,7)),\n",
    "            },\n",
    "        },\n",
    "        'scales':{\n",
    "            'HT4':{\n",
    "                'definition':'HTo4/1.',\n",
    "                'label':r'$H_T/4$',\n",
    "            },\n",
    "        },\n",
    "        'jobs':{\n",
    "        }\n",
    "    },\n",
    "    'pp_aa_8000':{\n",
    "        'orders':[\"LO\",\"NLO\",\"NNLO\"],\n",
    "        'nc_power':1,\n",
    "        'as_power':0,\n",
    "        'label':r'$pp \\to \\gamma \\gamma$ LHC @ 8 TeV',\n",
    "        'observables':{\n",
    "            'maa':{\n",
    "                'definition':'sqrt((p_a1_0+p_a2_0)**2-(p_a1_1+p_a2_1)**2-(p_a1_2+p_a2_2)**2-(p_a1_3+p_a2_3)**2)',\n",
    "                'label':r'$m(\\gamma\\gamma)$ [GeV]',\n",
    "                'binning':list(np.logspace(np.log10(100.),np.log10(1000.),10)),\n",
    "            },\n",
    "            'pTa1':{\n",
    "                'definition':'sqrt(p_a1_1**2 + p_a1_2**2)',\n",
    "                'label':r'$p_T(\\gamma_1)$ [GeV]',\n",
    "                'binning':list(np.logspace(np.log10(50.),np.log10(400.),10)),\n",
    "            },\n",
    "            'yaa':{\n",
    "                'definition':'0.5*log((p_a1_0+p_a2_0 + p_a1_3+p_a2_3)/(p_a1_0+p_a2_0 - p_a1_3-p_a2_3))',\n",
    "                'label':r'$y(\\gamma\\gamma)$',\n",
    "                'binning':list(np.linspace(-2.4,2.4,11)),\n",
    "            },\n",
    "        },\n",
    "        'scales':{\n",
    "            'mgg':{\n",
    "                'definition':'sqrt((p_a1_0+p_a2_0)**2-(p_a1_1+p_a2_1)**2-(p_a1_2+p_a2_2)**2-(p_a1_3+p_a2_3)**2)',\n",
    "                'label':r'$m(\\gamma\\gamma)/2$',\n",
    "            },\n",
    "            'HTh':{\n",
    "                'definition':'(sqrt(p_a1_1**2 + p_a1_2**2)+sqrt(p_a2_1**2 + p_a2_2**2)/2.)',\n",
    "                'label':r'$H_T/2$',\n",
    "            },\n",
    "\n",
    "        },\n",
    "        'jobs':{\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "# some preprocessing of above input\n",
    "for proc in processes.keys():\n",
    "  for ob in processes[proc]['observables']:\n",
    "    cur_edges = np.array(processes[proc]['observables'][ob]['binning'])\n",
    "    cur_edges_resc = (cur_edges- cur_edges[0])/(cur_edges[-1]-cur_edges[0])\n",
    "    # list of rescaled bin midpoints\n",
    "    processes[proc]['observables'][ob]['avgx']   = [ (cur_edges_resc[it]+cur_edges_resc[it+1])/2. for it in range(0,len(cur_edges_resc)-1) ]\n",
    "    processes[proc]['observables'][ob]['widthx'] = [ (cur_edges_resc[it+1]-cur_edges_resc[it]) for it in range(0,len(cur_edges_resc)-1) ]\n",
    "\n",
    "    cur_edges_resc = (cur_edges- (cur_edges[0]+cur_edges[-1])/2.)/(cur_edges[-1]-cur_edges[0])*2\n",
    "    processes[proc]['observables'][ob]['avgx_2']   = [ (cur_edges_resc[it]+cur_edges_resc[it+1])/2. for it in range(0,len(cur_edges_resc)-1) ]\n",
    "    processes[proc]['observables'][ob]['widthx_2'] = [ (cur_edges_resc[it+1]-cur_edges_resc[it]) for it in range(0,len(cur_edges_resc)-1) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRyPbp2mzJZR"
   },
   "outputs": [],
   "source": [
    "# Dynamically compute the average scale values per bin as two-dimensional histograms\n",
    "scale_bins = list(np.linspace(0,1000,21))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeHHolFzvstM"
   },
   "source": [
    "## HighTEA requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "_KivcmnBzgMt",
    "outputId": "8b493880-0fea-46d1-a067-eedd83720886"
   },
   "outputs": [],
   "source": [
    "# do actual hightea jobs\n",
    "for proc in processes.keys():\n",
    "  for order in processes[proc][\"orders\"]:\n",
    "    for scale in processes[proc][\"scales\"].keys():\n",
    "\n",
    "      #process and order\n",
    "      job = hightea('tnp-'+proc+\"-\"+order+'-'+scale,directory=USERDIR,overwrite=False)\n",
    "      job.process(proc,verbose=False)\n",
    "      job.contribution(order)\n",
    "\n",
    "      #scales\n",
    "      job.scales(scale,scale)\n",
    "      job.scale_variation('3-point')\n",
    "      job.define_new_variable(scale,processes[proc][\"scales\"][scale]['definition'])\n",
    "\n",
    "      #observables\n",
    "      for ob in processes[proc]['observables']:\n",
    "        job.define_new_variable(ob,processes[proc]['observables'][ob]['definition'])\n",
    "        job.observable(ob,processes[proc]['observables'][ob]['binning'])\n",
    "\n",
    "      #additional histograms for dynamical scale dependence\n",
    "      if order == 'LO':\n",
    "        for ob in processes[proc]['observables']:\n",
    "          job.observable([ob,scale],[processes[proc]['observables'][ob]['binning'],scale_bins])\n",
    "      job.request()\n",
    "      processes[proc]['jobs'][order+\"_\"+scale] = job\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_-cwSuRvyvi"
   },
   "source": [
    "## Post processing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eo1khCnyU5-z"
   },
   "outputs": [],
   "source": [
    "# compute dynamical scale averages and alpha_s for differential observables\n",
    "for proc in processes.keys():\n",
    "  for scale in processes[proc][\"scales\"]:\n",
    "    n_obs = len(processes[proc][\"observables\"])\n",
    "    for it, ob in enumerate(processes[proc][\"observables\"]):\n",
    "      cur_hist = processes[proc]['jobs'][\"LO_\"+scale]._requests[0]['result']['histograms'][it]\n",
    "      cur_hist_mu = processes[proc]['jobs'][\"LO_\"+scale]._requests[0]['result']['histograms'][it+n_obs]\n",
    "\n",
    "      # iteratre over observable bins\n",
    "      mean_by_bin = []\n",
    "      for cur_edges in cur_hist['binning']:\n",
    "        sum_scale_xsec = 0.\n",
    "        for cur_edge_mu in cur_hist_mu['binning']:\n",
    "          if (cur_edge_mu['edges'][0]['min_value'] == cur_edges['edges'][0]['min_value'] and\n",
    "              cur_edge_mu['edges'][0]['max_value'] == cur_edges['edges'][0]['max_value']):\n",
    "            mean_mu = (cur_edge_mu['edges'][1]['min_value']+ cur_edge_mu['edges'][1]['max_value'])/2.\n",
    "            sum_scale_xsec += mean_mu*cur_edge_mu['mean']/cur_edges['mean']\n",
    "        mean_by_bin.append(sum_scale_xsec)\n",
    "      processes[proc][\"observables\"][ob][\"avgmu_\"+scale] = mean_by_bin\n",
    "      processes[proc][\"observables\"][ob][\"alphas_\"+scale] = get_alphas(mean_by_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iMalEkUub_gZ"
   },
   "outputs": [],
   "source": [
    "# compute input for theory estimates\n",
    "for proc in processes.keys():\n",
    "  for scale in processes[proc][\"scales\"]:\n",
    "    tot_LO = processes[proc]['jobs'][processes[proc]['orders'][0]+\"_\"+scale].result()['fiducial_mean']\n",
    "    tot_NLO = processes[proc]['jobs'][processes[proc]['orders'][1]+\"_\"+scale].result()['fiducial_mean']\n",
    "    tot_NNLO = processes[proc]['jobs'][processes[proc]['orders'][2]+\"_\"+scale].result()['fiducial_mean']\n",
    "\n",
    "    for it, ob in enumerate(processes[proc][\"observables\"]):\n",
    "\n",
    "      res_LO = Run(processes[proc]['jobs'][processes[proc]['orders'][0]+\"_\"+scale].result(),nhist=it)\n",
    "      res_NLO = Run(processes[proc]['jobs'][processes[proc]['orders'][1]+\"_\"+scale].result(),nhist=it)\n",
    "      res_NNLO = Run(processes[proc]['jobs'][processes[proc]['orders'][2]+\"_\"+scale].result(),nhist=it)\n",
    "\n",
    "      processes[proc][\"observables\"][ob][\"res_\"+scale+\"_LO\"] = res_LO\n",
    "      processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"] = res_NLO\n",
    "      processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"] = res_NNLO\n",
    "\n",
    "      # compute rescaled sigmab for central prediction\n",
    "      id_central = 0\n",
    "      ncp = processes[proc][\"nc_power\"]\n",
    "      asp = processes[proc][\"as_power\"]\n",
    "      dyn_as = processes[proc][\"observables\"][ob][\"alphas_\"+scale]\n",
    "      overall_prefactor = (Nc**ncp) * (dyn_as**asp)\n",
    "\n",
    "      res_dNLO = res_NLO - res_LO\n",
    "      res_dNNLO = res_NNLO - res_NLO\n",
    "\n",
    "      sigmab0 = res_LO.values[:,id_central]/overall_prefactor\n",
    "      sigmab1 = res_dNLO.values[:,id_central]/overall_prefactor/dyn_as/Nc\n",
    "      sigmab2 = res_dNNLO.values[:,id_central]/overall_prefactor/(dyn_as**2)/(Nc**2)\n",
    "\n",
    "      processes[proc][\"observables\"][ob][\"sigmab0_\"+scale] = sigmab0\n",
    "      processes[proc][\"observables\"][ob][\"sigmab1_\"+scale] = sigmab1\n",
    "      processes[proc][\"observables\"][ob][\"sigmab2_\"+scale] = sigmab2\n",
    "        \n",
    "      processes[proc]['observables'][ob]['sigmab0_fracsqerr_'+scale] = np.square(np.divide(processes[proc]['observables'][ob]['res_'+scale+'_LO'].errors[:,0],processes[proc]['observables'][ob]['res_'+scale+'_LO'].values[:,0]))\n",
    "      processes[proc]['observables'][ob]['sigmab1_fracsqerr_'+scale] = np.divide(np.square(processes[proc]['observables'][ob]['res_'+scale+'_NLO'].errors[:,0])+np.square(processes[proc]['observables'][ob]['res_'+scale+'_LO'].errors[:,0]),np.square((processes[proc]['observables'][ob]['res_'+scale+'_NLO'].values[:,0]-processes[proc]['observables'][ob]['res_'+scale+'_LO'].values[:,0])))\n",
    "      processes[proc]['observables'][ob]['sigmab2_fracsqerr_'+scale] = np.divide(np.square(processes[proc]['observables'][ob]['res_'+scale+'_NNLO'].errors[:,0])+np.square(processes[proc]['observables'][ob]['res_'+scale+'_NLO'].errors[:,0]),np.square((processes[proc]['observables'][ob]['res_'+scale+'_NNLO'].values[:,0]-processes[proc]['observables'][ob]['res_'+scale+'_NLO'].values[:,0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfTqfOhawM-s"
   },
   "source": [
    "# TNP Uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rn.seed(20241212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sym_quad_variations(nPar,width):\n",
    "    base = np.zeros(nPar)\n",
    "    variations = []\n",
    "    for it in range(0,nPar):\n",
    "        new_var = base.copy()\n",
    "        new_var[it] += width\n",
    "        variations.append(new_var)\n",
    "    return np.array(variations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RyAZku9v7KsZ"
   },
   "source": [
    "## Default parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Sk2uZOSgZci"
   },
   "outputs": [],
   "source": [
    "# default implementation of TNP model for differential observables implementing with Bernstein polynomials\n",
    "tnp_n_par = 3\n",
    "tnp_n_sample = 1000\n",
    "\n",
    "def f_x(theta,x):\n",
    "    bp = BPoly([ [xx] for xx in theta], [0.,1.])\n",
    "    return bp(x)\n",
    "\n",
    "def sigmab_N_NNLO(theta,previous_data,x,aS):\n",
    "    return (previous_data[1]/previous_data[0])*(f_x(theta,x))*previous_data[0]*aS**2*Nc**2\n",
    "\n",
    "def sigmab_N_NNNLO(theta,previous_data,x,aS):\n",
    "    return ((previous_data[2]/previous_data[0])*(f_x(theta[:tnp_n_par],x))+(previous_data[1]/previous_data[0])*(f_x(theta[tnp_n_par:],x)))*previous_data[0]*aS**3*Nc**3\n",
    "\n",
    "# compute TNP uncertainty estimates\n",
    "for proc in processes.keys():\n",
    "  for scale in processes[proc][\"scales\"]:\n",
    "    for it, ob in enumerate(processes[proc][\"observables\"]):\n",
    "\n",
    "      prev_data_NLO  = np.array([processes[proc][\"observables\"][ob][\"sigmab0_\"+scale],\n",
    "                                 processes[proc][\"observables\"][ob][\"sigmab1_\"+scale]])\n",
    "      prev_data_NNLO  = np.array([processes[proc][\"observables\"][ob][\"sigmab0_\"+scale],\n",
    "                                  processes[proc][\"observables\"][ob][\"sigmab1_\"+scale],\n",
    "                                  processes[proc][\"observables\"][ob][\"sigmab2_\"+scale]])\n",
    "\n",
    "      avgx = processes[proc][\"observables\"][ob]['avgx']\n",
    "      avg_as = processes[proc][\"observables\"][ob][\"alphas_\"+scale]\n",
    "\n",
    "      ncp = processes[proc][\"nc_power\"]\n",
    "      asp = processes[proc][\"as_power\"]\n",
    "      overall_prefactor = (Nc**ncp) * (avg_as**asp)\n",
    "\n",
    "      # estimate the NLO uncertainty\n",
    "      all_est_NNLO = []\n",
    "      for it in range(0,tnp_n_sample):\n",
    "        cur_theta = [rn.uniform(-1,1) for xx in range(0,tnp_n_par)]\n",
    "        est_NNLO = (processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].values[:,0]\n",
    "                    +overall_prefactor*sigmab_N_NNLO(cur_theta,prev_data_NLO,avgx,avg_as))\n",
    "        all_est_NNLO.append(est_NNLO)\n",
    "      processes[proc][\"observables\"][ob][\"tnp_\"+scale+\"_NLO_max\"] = np.amax(np.array(all_est_NNLO),axis=0)\n",
    "      processes[proc][\"observables\"][ob][\"tnp_\"+scale+\"_NLO_min\"] = np.amin(np.array(all_est_NNLO),axis=0)\n",
    "\n",
    "      # estimate the NNLO uncertainty\n",
    "      all_est_NNNLO = []\n",
    "      for it in range(0,tnp_n_sample):\n",
    "        cur_theta = [rn.uniform(-1,1) for xx in range(0,tnp_n_par*2)]\n",
    "        est_NNNLO = (processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"].values[:,0]\n",
    "                     +overall_prefactor*sigmab_N_NNNLO(cur_theta,prev_data_NNLO,avgx,avg_as))\n",
    "        all_est_NNNLO.append(est_NNNLO)\n",
    "      processes[proc][\"observables\"][ob][\"tnp_\"+scale+\"_NNLO_max\"] = np.amax(np.array(all_est_NNNLO),axis=0)\n",
    "      processes[proc][\"observables\"][ob][\"tnp_\"+scale+\"_NNLO_min\"] = np.amin(np.array(all_est_NNNLO),axis=0)\n",
    "\n",
    "      # estimates from quad method\n",
    "      quad_variations = create_sym_quad_variations(tnp_n_par,1.)\n",
    "      n_quad_var = len(quad_variations)\n",
    "      all_est_NNLO_quad = []\n",
    "      for it in range(0,n_quad_var):\n",
    "        cur_theta = quad_variations[it]\n",
    "        err_NNLO = (overall_prefactor*sigmab_N_NNLO(cur_theta,prev_data_NLO,avgx,avg_as))\n",
    "        all_est_NNLO_quad.append(err_NNLO**2)\n",
    "      err_NNLO = np.sqrt(sum(all_est_NNLO_quad))\n",
    "      processes[proc][\"observables\"][ob][\"tnp_\"+scale+\"_NLO_quad_max\"] = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].values[:,0] + err_NNLO\n",
    "      processes[proc][\"observables\"][ob][\"tnp_\"+scale+\"_NLO_quad_min\"] = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].values[:,0] - err_NNLO\n",
    "\n",
    "      quad_variations = create_sym_quad_variations(2*tnp_n_par,1.)\n",
    "      n_quad_var = len(quad_variations)\n",
    "      all_est_NNNLO_quad = []\n",
    "      for it in range(0,n_quad_var):\n",
    "        cur_theta = quad_variations[it]\n",
    "        err_NNNLO = (overall_prefactor*sigmab_N_NNNLO(cur_theta,prev_data_NNLO,avgx,avg_as))\n",
    "        all_est_NNNLO_quad.append(err_NNNLO**2)\n",
    "        \n",
    "      err_NNNLO = np.sqrt(sum(all_est_NNNLO_quad))\n",
    "      processes[proc][\"observables\"][ob][\"tnp_\"+scale+\"_NNLO_quad_max\"] = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"].values[:,0] + err_NNNLO\n",
    "      processes[proc][\"observables\"][ob][\"tnp_\"+scale+\"_NNLO_quad_min\"] = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"].values[:,0] - err_NNNLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chebyshev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnp_n_par = 3\n",
    "tnp_n_sample = 1000\n",
    "\n",
    "def g_x(theta,x):\n",
    "    return 0.5*(theta[0]+theta[1]*x+theta[2]*(2.*x**2 - 1))\n",
    "\n",
    "def sigmab_N_NNLO_cheby(theta,previous_data,x,aS):\n",
    "    return (previous_data[1]/previous_data[0])*(g_x(theta,x))*previous_data[0]*aS**2*Nc**2\n",
    "\n",
    "def sigmab_N_NNNLO_cheby(theta,previous_data,x,aS):\n",
    "    return ((previous_data[2]/previous_data[0])*(g_x(theta[:tnp_n_par],x))+(previous_data[1]/previous_data[0])*(g_x(theta[tnp_n_par:],x)))*previous_data[0]*aS**3*Nc**3\n",
    "\n",
    "# compute TNP uncertainty estimates\n",
    "for proc in processes.keys():\n",
    "  for scale in processes[proc][\"scales\"]:\n",
    "    for it, ob in enumerate(processes[proc][\"observables\"]):\n",
    "\n",
    "      prev_data_NLO  = np.array([processes[proc][\"observables\"][ob][\"sigmab0_\"+scale],\n",
    "                                 processes[proc][\"observables\"][ob][\"sigmab1_\"+scale]])\n",
    "      prev_data_NNLO  = np.array([processes[proc][\"observables\"][ob][\"sigmab0_\"+scale],\n",
    "                                  processes[proc][\"observables\"][ob][\"sigmab1_\"+scale],\n",
    "                                  processes[proc][\"observables\"][ob][\"sigmab2_\"+scale]])\n",
    "\n",
    "      avgx = processes[proc][\"observables\"][ob]['avgx_2']\n",
    "      avg_as = processes[proc][\"observables\"][ob][\"alphas_\"+scale]\n",
    "\n",
    "      ncp = processes[proc][\"nc_power\"]\n",
    "      asp = processes[proc][\"as_power\"]\n",
    "      overall_prefactor = (Nc**ncp) * (avg_as**asp)\n",
    "\n",
    "      # estimate the NLO uncertainty\n",
    "      all_est_NNLO = []\n",
    "      for it in range(0,tnp_n_sample):\n",
    "        cur_theta = [rn.uniform(-1,1) for xx in range(0,tnp_n_par)]\n",
    "        est_NNLO = (processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].values[:,0]\n",
    "                    +overall_prefactor*sigmab_N_NNLO_cheby(cur_theta,prev_data_NLO,np.array(avgx),avg_as))\n",
    "        all_est_NNLO.append(est_NNLO)\n",
    "      processes[proc][\"observables\"][ob][\"tnp_cheby_\"+scale+\"_NLO_max\"] = np.amax(np.array(all_est_NNLO),axis=0)\n",
    "      processes[proc][\"observables\"][ob][\"tnp_cheby_\"+scale+\"_NLO_min\"] = np.amin(np.array(all_est_NNLO),axis=0)\n",
    "\n",
    "      # estimate the NNLO uncertainty\n",
    "      all_est_NNNLO = []\n",
    "      for it in range(0,tnp_n_sample):\n",
    "        cur_theta = [rn.uniform(-1,1) for xx in range(0,tnp_n_par*2)]\n",
    "        est_NNNLO = (processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"].values[:,0]\n",
    "                     +overall_prefactor*sigmab_N_NNNLO_cheby(cur_theta,prev_data_NNLO,np.array(avgx),avg_as))\n",
    "        all_est_NNNLO.append(est_NNNLO)\n",
    "      processes[proc][\"observables\"][ob][\"tnp_cheby_\"+scale+\"_NNLO_max\"] = np.amax(np.array(all_est_NNNLO),axis=0)\n",
    "      processes[proc][\"observables\"][ob][\"tnp_cheby_\"+scale+\"_NNLO_min\"] = np.amin(np.array(all_est_NNNLO),axis=0)\n",
    "\n",
    "      # estimates from quad method\n",
    "      quad_variations = create_sym_quad_variations(tnp_n_par,1.)\n",
    "      n_quad_var = len(quad_variations)\n",
    "      all_est_NNLO_quad = []\n",
    "      for it in range(0,n_quad_var):\n",
    "        cur_theta = quad_variations[it]\n",
    "        err_NNLO = (overall_prefactor*sigmab_N_NNLO_cheby(cur_theta,prev_data_NLO,np.array(avgx),avg_as))\n",
    "        all_est_NNLO_quad.append(err_NNLO**2)\n",
    "      err_NNLO = np.sqrt(sum(all_est_NNLO_quad))\n",
    "      processes[proc][\"observables\"][ob][\"tnp_cheby_\"+scale+\"_NLO_quad_max\"] = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].values[:,0] + err_NNLO\n",
    "      processes[proc][\"observables\"][ob][\"tnp_cheby_\"+scale+\"_NLO_quad_min\"] = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].values[:,0] - err_NNLO\n",
    "\n",
    "      quad_variations = create_sym_quad_variations(2*tnp_n_par,1.)\n",
    "      n_quad_var = len(quad_variations)\n",
    "      all_est_NNNLO_quad = []\n",
    "      for it in range(0,n_quad_var):\n",
    "        cur_theta = quad_variations[it]\n",
    "        err_NNNLO = (overall_prefactor*sigmab_N_NNNLO_cheby(cur_theta,prev_data_NNLO,np.array(avgx),avg_as))\n",
    "        all_est_NNNLO_quad.append(err_NNNLO**2)\n",
    "        \n",
    "      err_NNNLO = np.sqrt(sum(all_est_NNNLO_quad))\n",
    "      processes[proc][\"observables\"][ob][\"tnp_cheby_\"+scale+\"_NNLO_quad_max\"] = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"].values[:,0] + err_NNNLO\n",
    "      processes[proc][\"observables\"][ob][\"tnp_cheby_\"+scale+\"_NNLO_quad_min\"] = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"].values[:,0] - err_NNNLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qLD58aNzl5i"
   },
   "source": [
    "## Alternative model lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1HKVJTc3QuhB"
   },
   "outputs": [],
   "source": [
    "# alternative implementation of TNP model for differential observables\n",
    "tnp_n_par = 2\n",
    "tnp_n_sample = 1000\n",
    "\n",
    "def f_x_alt(theta,x):\n",
    "    bp = BPoly([ [xx] for xx in theta], [0.,1.])\n",
    "    return bp(x)\n",
    "\n",
    "def sigmab_N_NNLO_alt(theta,previous_data,x,aS):\n",
    "    return (previous_data[1]/previous_data[0])*(f_x_alt(theta,x))*previous_data[0]*aS**2*Nc**2\n",
    "\n",
    "def sigmab_N_NNNLO_alt(theta,previous_data,x,aS):\n",
    "    return ((previous_data[2]/previous_data[0])*(f_x_alt(theta[:tnp_n_par],x))+(previous_data[1]/previous_data[0])*(f_x_alt(theta[tnp_n_par:],x)))*previous_data[0]*aS**3*Nc**3\n",
    "\n",
    "# compute TNP uncertainty estimates alt\n",
    "for proc in processes.keys():\n",
    "  for scale in processes[proc][\"scales\"]:\n",
    "    for it, ob in enumerate(processes[proc][\"observables\"]):\n",
    "\n",
    "      prev_data_NLO  = np.array([processes[proc][\"observables\"][ob][\"sigmab0_\"+scale],\n",
    "                                 processes[proc][\"observables\"][ob][\"sigmab1_\"+scale]])\n",
    "      prev_data_NNLO  = np.array([processes[proc][\"observables\"][ob][\"sigmab0_\"+scale],\n",
    "                                  processes[proc][\"observables\"][ob][\"sigmab1_\"+scale],\n",
    "                                  processes[proc][\"observables\"][ob][\"sigmab2_\"+scale]])\n",
    "\n",
    "      avgx = processes[proc][\"observables\"][ob]['avgx']\n",
    "      avg_as = processes[proc][\"observables\"][ob][\"alphas_\"+scale]\n",
    "\n",
    "      ncp = processes[proc][\"nc_power\"]\n",
    "      asp = processes[proc][\"as_power\"]\n",
    "      overall_prefactor = (Nc**ncp) * (avg_as**asp)\n",
    "\n",
    "      # estimate the NLO uncertainty\n",
    "      all_est_NNLO = []\n",
    "      for it in range(0,tnp_n_sample):\n",
    "        cur_theta = [rn.uniform(-1,1) for xx in range(0,tnp_n_par)]\n",
    "        est_NNLO = (processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].values[:,0]\n",
    "                    +overall_prefactor*sigmab_N_NNLO_alt(cur_theta,prev_data_NLO,avgx,avg_as))\n",
    "        all_est_NNLO.append(est_NNLO)\n",
    "      processes[proc][\"observables\"][ob][\"tnp_alt_\"+scale+\"_NLO_max\"] = np.amax(np.array(all_est_NNLO),axis=0)\n",
    "      processes[proc][\"observables\"][ob][\"tnp_alt_\"+scale+\"_NLO_min\"] = np.amin(np.array(all_est_NNLO),axis=0)\n",
    "\n",
    "      # estimate the NNLO uncertainty\n",
    "      all_est_NNNLO = []\n",
    "      for it in range(0,tnp_n_sample):\n",
    "        cur_theta = [rn.uniform(-1,1) for xx in range(0,tnp_n_par*2)]\n",
    "        est_NNNLO = (processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"].values[:,0]\n",
    "                     +overall_prefactor*sigmab_N_NNNLO_alt(cur_theta,prev_data_NNLO,avgx,avg_as))\n",
    "        all_est_NNNLO.append(est_NNNLO)\n",
    "      processes[proc][\"observables\"][ob][\"tnp_alt_\"+scale+\"_NNLO_max\"] = np.amax(np.array(all_est_NNNLO),axis=0)\n",
    "      processes[proc][\"observables\"][ob][\"tnp_alt_\"+scale+\"_NNLO_min\"] = np.amin(np.array(all_est_NNNLO),axis=0)\n",
    "\n",
    "      # estimates from quad method\n",
    "      quad_variations = create_sym_quad_variations(tnp_n_par,1.)\n",
    "      n_quad_var = len(quad_variations)\n",
    "      all_est_NNLO_quad = []\n",
    "      for it in range(0,n_quad_var):\n",
    "        cur_theta = quad_variations[it]\n",
    "        err_NNLO = (overall_prefactor*sigmab_N_NNLO_alt(cur_theta,prev_data_NLO,avgx,avg_as))\n",
    "        all_est_NNLO_quad.append(err_NNLO**2)\n",
    "      err_NNLO = np.sqrt(sum(all_est_NNLO_quad))\n",
    "      processes[proc][\"observables\"][ob][\"tnp_alt_\"+scale+\"_NLO_quad_max\"] = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].values[:,0] + err_NNLO\n",
    "      processes[proc][\"observables\"][ob][\"tnp_alt_\"+scale+\"_NLO_quad_min\"] = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].values[:,0] - err_NNLO\n",
    "\n",
    "      quad_variations = create_sym_quad_variations(2*tnp_n_par,1.)\n",
    "      n_quad_var = len(quad_variations)\n",
    "      all_est_NNNLO_quad = []\n",
    "      for it in range(0,n_quad_var):\n",
    "        cur_theta = quad_variations[it]\n",
    "        err_NNNLO = (overall_prefactor*sigmab_N_NNNLO_alt(cur_theta,prev_data_NNLO,avgx,avg_as))\n",
    "        all_est_NNNLO_quad.append(err_NNNLO**2)\n",
    "        \n",
    "      err_NNNLO = np.sqrt(sum(all_est_NNNLO_quad))\n",
    "      processes[proc][\"observables\"][ob][\"tnp_alt_\"+scale+\"_NNLO_quad_max\"] = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"].values[:,0] + err_NNNLO\n",
    "      processes[proc][\"observables\"][ob][\"tnp_alt_\"+scale+\"_NNLO_quad_min\"] = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"].values[:,0] - err_NNNLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IlGiwoSv4TU"
   },
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIlGdzMgQgam"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n",
    "\n",
    "# plotting helper function\n",
    "def get_ll_from_data(data_e,data_y):\n",
    "    new_x = []\n",
    "    new_y = []\n",
    "    for it in range(0,len(data_y)):\n",
    "      new_x.append(data_e[it])\n",
    "      new_x.append(data_e[it+1])\n",
    "      new_y.append(data_y[it])\n",
    "      new_y.append(data_y[it])\n",
    "    return np.array([new_x,new_y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfjYzMacz4do"
   },
   "source": [
    "## Plots default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3cp5W05SL1qH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for proc in processes:\n",
    "  for scale in processes[proc][\"scales\"]:\n",
    "\n",
    "    pp = PdfPages('./pdfs/hightea_'+proc+\"_\"+scale+'.pdf')\n",
    "\n",
    "    n_obs = len(processes[proc][\"observables\"])\n",
    "\n",
    "    if n_obs == 1:\n",
    "        fig, ax = plt.subplots(3, 2, figsize=(10./3.*2,6.), constrained_layout=True)#\n",
    "    else:\n",
    "        fig, ax = plt.subplots(3, n_obs, figsize=(10./3.*n_obs,6.), constrained_layout=True)#\n",
    "\n",
    "    fig.suptitle(processes[proc]['label']+r' central scale: $\\mu = $ '+processes[proc]['scales'][scale]['label']+\n",
    "                 \" Bernstein parameterisation (k=2)\")\n",
    "    y_range_min_common = 10.\n",
    "    y_range_max_common = 0.\n",
    "    for it, ob in enumerate(processes[proc][\"observables\"]):\n",
    "\n",
    "        # data preparation\n",
    "\n",
    "        norm = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].values[:,0]\n",
    "\n",
    "        # statistical uncertainties of central prediction\n",
    "        error_x_val = np.array(processes[proc][\"observables\"][ob]['binning'])\n",
    "        error_x_val = (error_x_val[:-1]+error_x_val[1:])/2.\n",
    "        error_cen_NLO  = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].errors[:,0]/norm\n",
    "        error_cen_NNLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"].errors[:,0]/norm\n",
    "\n",
    "        ll_error_cen_nlo  = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],error_cen_NLO)\n",
    "        ll_error_cen_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],error_cen_NNLO)\n",
    "        \n",
    "        # tnp errors band\n",
    "        tnp_band_cen_NLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].values[:,0]/norm\n",
    "        tnp_band_max_NLO = processes[proc][\"observables\"][ob][\"tnp_\"+scale+\"_NLO_max\"]/norm\n",
    "        tnp_band_min_NLO = processes[proc][\"observables\"][ob][\"tnp_\"+scale+\"_NLO_min\"]/norm\n",
    "\n",
    "        tnp_band_cen_NNLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"].values[:,0]/norm\n",
    "        tnp_band_max_NNLO = processes[proc][\"observables\"][ob][\"tnp_\"+scale+\"_NNLO_max\"]/norm\n",
    "        tnp_band_min_NNLO = processes[proc][\"observables\"][ob][\"tnp_\"+scale+\"_NNLO_min\"]/norm\n",
    "\n",
    "        ll_tnp_cen_nlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_band_cen_NLO)\n",
    "        ll_tnp_max_nlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_band_max_NLO)\n",
    "        ll_tnp_min_nlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_band_min_NLO)\n",
    "\n",
    "        ll_tnp_cen_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_band_cen_NNLO)\n",
    "        ll_tnp_max_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_band_max_NNLO)\n",
    "        ll_tnp_min_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_band_min_NNLO)\n",
    "\n",
    "        # tnp quad\n",
    "        tnp_quad_cen_NLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].values[:,0]/norm\n",
    "        tnp_quad_max_NLO = processes[proc][\"observables\"][ob][\"tnp_\"+scale+\"_NLO_quad_max\"]/norm\n",
    "        tnp_quad_min_NLO = processes[proc][\"observables\"][ob][\"tnp_\"+scale+\"_NLO_quad_min\"]/norm\n",
    "\n",
    "        tnp_quad_cen_NNLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"].values[:,0]/norm\n",
    "        tnp_quad_max_NNLO = processes[proc][\"observables\"][ob][\"tnp_\"+scale+\"_NNLO_quad_max\"]/norm\n",
    "        tnp_quad_min_NNLO = processes[proc][\"observables\"][ob][\"tnp_\"+scale+\"_NNLO_quad_min\"]/norm\n",
    "\n",
    "        ll_tnp_cen_quad_nlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_quad_cen_NLO)\n",
    "        ll_tnp_max_quad_nlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_quad_max_NLO)\n",
    "        ll_tnp_min_quad_nlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_quad_min_NLO)\n",
    "\n",
    "        ll_tnp_cen_quad_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_quad_cen_NNLO)\n",
    "        ll_tnp_max_quad_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_quad_max_NNLO)\n",
    "        ll_tnp_min_quad_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_quad_min_NNLO)\n",
    "        \n",
    "        # scale uncertainties\n",
    "        var_band_cen_LO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_LO\"].values[:,0]/norm\n",
    "        var_band_max_LO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_LO\"].values[:,2]/norm\n",
    "        var_band_min_LO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_LO\"].values[:,1]/norm\n",
    "\n",
    "        var_band_cen_NLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].values[:,0]/norm\n",
    "        var_band_max_NLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].values[:,2]/norm\n",
    "        var_band_min_NLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].values[:,1]/norm\n",
    "\n",
    "        var_band_cen_NNLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"].values[:,0]/norm\n",
    "        var_band_max_NNLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"].values[:,2]/norm\n",
    "        var_band_min_NNLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"].values[:,1]/norm\n",
    "\n",
    "        ll_var_cen_lo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],var_band_cen_LO)\n",
    "        ll_var_max_lo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],var_band_max_LO)\n",
    "        ll_var_min_lo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],var_band_min_LO)\n",
    "\n",
    "        ll_var_cen_nlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],var_band_cen_NLO)\n",
    "        ll_var_max_nlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],var_band_max_NLO)\n",
    "        ll_var_min_nlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],var_band_min_NLO)\n",
    "\n",
    "        ll_var_cen_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],var_band_cen_NNLO)\n",
    "        ll_var_max_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],var_band_max_NNLO)\n",
    "        ll_var_min_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],var_band_min_NNLO)\n",
    "\n",
    "\n",
    "        # general layout\n",
    "        if it == 0:\n",
    "            ax[1][it].set_ylabel('ratio to NLO')\n",
    "            ax[0][it].tick_params(labelbottom=False,bottom=False)\n",
    "            ax[1][it].tick_params(labelbottom=False,bottom=False)\n",
    "        else:\n",
    "            ax[0][it].tick_params(labelbottom = False, bottom = False, left = False, labelleft = False)\n",
    "            ax[1][it].tick_params(labelbottom = False, bottom = False, left = False, labelleft = False)\n",
    "            ax[2][it].tick_params(left = False, labelleft = False)\n",
    "\n",
    "        # TNP uncertainty with band method\n",
    "        ax[0][it].plot(ll_var_cen_lo[0],ll_var_cen_lo[1],color='limegreen',label='LO')\n",
    "\n",
    "        ax[0][it].plot(ll_tnp_cen_nlo[0],ll_tnp_cen_nlo[1],\n",
    "                           color='blue',label='NLO')\n",
    "        ax[0][it].errorbar(error_x_val,var_band_cen_NLO,error_cen_NLO,color='blue',linestyle='none')\n",
    "\n",
    "        ax[0][it].fill_between(ll_tnp_cen_nlo[0],\n",
    "                               ll_tnp_min_nlo[1],\n",
    "                               ll_tnp_max_nlo[1],color='blue',alpha=0.3)\n",
    "\n",
    "        ax[0][it].plot(ll_tnp_cen_nnlo[0],ll_tnp_cen_nnlo[1],\n",
    "                       color='red',label='NNLO')\n",
    "        ax[0][it].errorbar(error_x_val,var_band_cen_NNLO,error_cen_NNLO,color='Red',linestyle='none')\n",
    "\n",
    "        ax[0][it].fill_between(ll_tnp_cen_nnlo[0],\n",
    "                               ll_tnp_min_nnlo[1],\n",
    "                               ll_tnp_max_nnlo[1],color='red',alpha=0.4)\n",
    "\n",
    "\n",
    "        ax[0][it].text(0.02,0.9,'Errorbands: TNP (bands)',transform=ax[0][it].transAxes)\n",
    "        ax[0][it].grid(True)\n",
    "\n",
    "        if it == 0: ax[0][it].legend(loc='lower left',ncol=3)\n",
    "\n",
    "        y_range_min = np.amin([ll_var_cen_lo[1],\n",
    "                               ll_tnp_min_nlo[1],ll_tnp_min_nnlo[1],\n",
    "                               ll_var_min_nlo[1],ll_var_min_nnlo[1]\n",
    "                               ])\n",
    "        y_range_max = np.amax([ll_var_cen_lo[1],\n",
    "                               ll_tnp_max_nlo[1],ll_tnp_max_nnlo[1],\n",
    "                               ll_var_max_nlo[1],ll_var_max_nnlo[1]\n",
    "                               ])\n",
    "        if y_range_min < y_range_min_common: y_range_min_common = y_range_min\n",
    "        if y_range_max > y_range_max_common: y_range_max_common = y_range_max\n",
    "\n",
    "        x_range_min = np.amin(ll_var_cen_lo[0])\n",
    "        x_range_max = np.amax(ll_var_cen_lo[0])\n",
    "        \n",
    "\n",
    "        ax[0][it].set_xlim(x_range_min,x_range_max)\n",
    "\n",
    "        # TNP with quad method\n",
    "        ax[1][it].plot(ll_var_cen_lo[0],ll_var_cen_lo[1],color='limegreen',label='LO')\n",
    "\n",
    "        ax[1][it].plot(ll_tnp_cen_nlo[0],ll_tnp_cen_nlo[1],\n",
    "                           color='blue',label='NLO')\n",
    "        ax[1][it].errorbar(error_x_val,var_band_cen_NLO,error_cen_NLO,color='blue',linestyle='none')\n",
    "\n",
    "        ax[1][it].fill_between(ll_tnp_cen_quad_nlo[0],\n",
    "                               ll_tnp_min_quad_nlo[1],\n",
    "                               ll_tnp_max_quad_nlo[1],color='blue',alpha=0.3)\n",
    "\n",
    "        ax[1][it].plot(ll_tnp_cen_quad_nnlo[0],ll_tnp_cen_quad_nnlo[1],\n",
    "                       color='red',label='NNLO')\n",
    "        ax[1][it].errorbar(error_x_val,var_band_cen_NNLO,error_cen_NNLO,color='Red',linestyle='none')\n",
    "\n",
    "        ax[1][it].fill_between(ll_tnp_cen_quad_nnlo[0],\n",
    "                               ll_tnp_min_quad_nnlo[1],\n",
    "                               ll_tnp_max_quad_nnlo[1],color='red',alpha=0.4)\n",
    "\n",
    "        ax[1][it].text(0.02,0.9,'Errorbands: TNP (quad.)',transform=ax[1][it].transAxes)\n",
    "        ax[1][it].grid(True)        \n",
    "        ax[1][it].set_xlim(x_range_min,x_range_max)\n",
    "\n",
    "\n",
    "        # scale variations\n",
    "        ax[2][it].plot(ll_var_cen_lo[0],ll_var_cen_lo[1],color='limegreen',label='LO')\n",
    "        ax[2][it].fill_between(ll_var_cen_lo[0],\n",
    "                               ll_var_min_lo[1],\n",
    "                               ll_var_max_lo[1],color='limegreen',alpha=0.3)\n",
    "\n",
    "        ax[2][it].plot(ll_var_cen_nlo[0],ll_var_cen_nlo[1],color='blue',label='NLO')\n",
    "        ax[2][it].errorbar(error_x_val,var_band_cen_NLO,error_cen_NLO,color='blue',linestyle='none')\n",
    "        ax[2][it].fill_between(ll_var_cen_nlo[0],\n",
    "                               ll_var_min_nlo[1],\n",
    "                               ll_var_max_nlo[1],color='blue',alpha=0.3)\n",
    "\n",
    "        ax[2][it].plot(ll_var_cen_nnlo[0],ll_var_cen_nnlo[1],color='red',label='NNLO')\n",
    "        ax[2][it].fill_between(ll_var_cen_nnlo[0],\n",
    "                               ll_var_min_nnlo[1],\n",
    "                               ll_var_max_nnlo[1],color='red',alpha=0.4)\n",
    "        ax[2][it].errorbar(error_x_val,var_band_cen_NNLO,error_cen_NNLO,color='Red',linestyle='none')\n",
    "\n",
    "        ax[2][it].text(0.02,0.9,'Errorbands: 3-Point scale var.',transform=ax[2][it].transAxes)\n",
    "        ax[2][it].grid(True)\n",
    "        ax[2][it].set_xlim(x_range_min,x_range_max)\n",
    "\n",
    "\n",
    "        ax[2][it].set_xlabel(processes[proc][\"observables\"][ob]['label'])\n",
    "    for it in range(0,len(processes[proc][\"observables\"])):\n",
    "      ax[0][it].set_ylim(y_range_min_common*0.8,y_range_max_common*1.2)\n",
    "      ax[1][it].set_ylim(y_range_min_common*0.8,y_range_max_common*1.2)\n",
    "      ax[2][it].set_ylim(y_range_min_common*0.8,y_range_max_common*1.2)\n",
    "    pp.savefig();\n",
    "    plt.show()\n",
    "    pp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots for Chebyshev parameterisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for proc in processes:\n",
    "  for scale in processes[proc][\"scales\"]:\n",
    "\n",
    "    pp = PdfPages('./pdfs/hightea_'+proc+\"_\"+scale+'-cheby.pdf')\n",
    "\n",
    "    n_obs = len(processes[proc][\"observables\"])\n",
    "\n",
    "    if n_obs == 1:\n",
    "        fig, ax = plt.subplots(3, 2, figsize=(10./3.*2,6.), constrained_layout=True)#\n",
    "    else:\n",
    "        fig, ax = plt.subplots(3, n_obs, figsize=(10./3.*n_obs,6.), constrained_layout=True)#\n",
    "\n",
    "    fig.suptitle(processes[proc]['label']+r' central scale: $\\mu = $ '+processes[proc]['scales'][scale]['label']+\n",
    "                 \" Chebyshev parameterisation (k=2)\")\n",
    "    y_range_min_common = 10.\n",
    "    y_range_max_common = 0.\n",
    "    for it, ob in enumerate(processes[proc][\"observables\"]):\n",
    "\n",
    "        # data preparation\n",
    "\n",
    "        norm = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].values[:,0]\n",
    "\n",
    "        # statistical uncertainties of central prediction\n",
    "        error_x_val = np.array(processes[proc][\"observables\"][ob]['binning'])\n",
    "        error_x_val = (error_x_val[:-1]+error_x_val[1:])/2.\n",
    "        error_cen_NLO  = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].errors[:,0]/norm\n",
    "        error_cen_NNLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"].errors[:,0]/norm\n",
    "\n",
    "        ll_error_cen_nlo  = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],error_cen_NLO)\n",
    "        ll_error_cen_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],error_cen_NNLO)\n",
    "        \n",
    "        # tnp errors band\n",
    "        tnp_band_cen_NLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].values[:,0]/norm\n",
    "        tnp_band_max_NLO = processes[proc][\"observables\"][ob][\"tnp_cheby_\"+scale+\"_NLO_max\"]/norm\n",
    "        tnp_band_min_NLO = processes[proc][\"observables\"][ob][\"tnp_cheby_\"+scale+\"_NLO_min\"]/norm\n",
    "\n",
    "        tnp_band_cen_NNLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"].values[:,0]/norm\n",
    "        tnp_band_max_NNLO = processes[proc][\"observables\"][ob][\"tnp_cheby_\"+scale+\"_NNLO_max\"]/norm\n",
    "        tnp_band_min_NNLO = processes[proc][\"observables\"][ob][\"tnp_cheby_\"+scale+\"_NNLO_min\"]/norm\n",
    "\n",
    "        ll_tnp_cen_nlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_band_cen_NLO)\n",
    "        ll_tnp_max_nlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_band_max_NLO)\n",
    "        ll_tnp_min_nlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_band_min_NLO)\n",
    "\n",
    "        ll_tnp_cen_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_band_cen_NNLO)\n",
    "        ll_tnp_max_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_band_max_NNLO)\n",
    "        ll_tnp_min_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_band_min_NNLO)\n",
    "\n",
    "        # tnp quad\n",
    "        tnp_quad_cen_NLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].values[:,0]/norm\n",
    "        tnp_quad_max_NLO = processes[proc][\"observables\"][ob][\"tnp_cheby_\"+scale+\"_NLO_quad_max\"]/norm\n",
    "        tnp_quad_min_NLO = processes[proc][\"observables\"][ob][\"tnp_cheby_\"+scale+\"_NLO_quad_min\"]/norm\n",
    "\n",
    "        tnp_quad_cen_NNLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"].values[:,0]/norm\n",
    "        tnp_quad_max_NNLO = processes[proc][\"observables\"][ob][\"tnp_cheby_\"+scale+\"_NNLO_quad_max\"]/norm\n",
    "        tnp_quad_min_NNLO = processes[proc][\"observables\"][ob][\"tnp_cheby_\"+scale+\"_NNLO_quad_min\"]/norm\n",
    "\n",
    "        ll_tnp_cen_quad_nlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_quad_cen_NLO)\n",
    "        ll_tnp_max_quad_nlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_quad_max_NLO)\n",
    "        ll_tnp_min_quad_nlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_quad_min_NLO)\n",
    "\n",
    "        ll_tnp_cen_quad_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_quad_cen_NNLO)\n",
    "        ll_tnp_max_quad_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_quad_max_NNLO)\n",
    "        ll_tnp_min_quad_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_quad_min_NNLO)\n",
    "        \n",
    "        # scale uncertainties\n",
    "        var_band_cen_LO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_LO\"].values[:,0]/norm\n",
    "        var_band_max_LO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_LO\"].values[:,2]/norm\n",
    "        var_band_min_LO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_LO\"].values[:,1]/norm\n",
    "\n",
    "        var_band_cen_NLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].values[:,0]/norm\n",
    "        var_band_max_NLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].values[:,2]/norm\n",
    "        var_band_min_NLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].values[:,1]/norm\n",
    "\n",
    "        var_band_cen_NNLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"].values[:,0]/norm\n",
    "        var_band_max_NNLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"].values[:,2]/norm\n",
    "        var_band_min_NNLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"].values[:,1]/norm\n",
    "\n",
    "        ll_var_cen_lo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],var_band_cen_LO)\n",
    "        ll_var_max_lo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],var_band_max_LO)\n",
    "        ll_var_min_lo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],var_band_min_LO)\n",
    "\n",
    "        ll_var_cen_nlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],var_band_cen_NLO)\n",
    "        ll_var_max_nlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],var_band_max_NLO)\n",
    "        ll_var_min_nlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],var_band_min_NLO)\n",
    "\n",
    "        ll_var_cen_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],var_band_cen_NNLO)\n",
    "        ll_var_max_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],var_band_max_NNLO)\n",
    "        ll_var_min_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],var_band_min_NNLO)\n",
    "\n",
    "\n",
    "        # general layout\n",
    "        if it == 0:\n",
    "            ax[1][it].set_ylabel('ratio to NLO')\n",
    "            ax[0][it].tick_params(labelbottom=False,bottom=False)\n",
    "            ax[1][it].tick_params(labelbottom=False,bottom=False)\n",
    "        else:\n",
    "            ax[0][it].tick_params(labelbottom = False, bottom = False, left = False, labelleft = False)\n",
    "            ax[1][it].tick_params(labelbottom = False, bottom = False, left = False, labelleft = False)\n",
    "            ax[2][it].tick_params(left = False, labelleft = False)\n",
    "\n",
    "        # TNP uncertainty with band method\n",
    "        ax[0][it].plot(ll_var_cen_lo[0],ll_var_cen_lo[1],color='limegreen',label='LO')\n",
    "\n",
    "        ax[0][it].plot(ll_tnp_cen_nlo[0],ll_tnp_cen_nlo[1],\n",
    "                           color='blue',label='NLO')\n",
    "        ax[0][it].errorbar(error_x_val,var_band_cen_NLO,error_cen_NLO,color='blue',linestyle='none')\n",
    "\n",
    "        ax[0][it].fill_between(ll_tnp_cen_nlo[0],\n",
    "                               ll_tnp_min_nlo[1],\n",
    "                               ll_tnp_max_nlo[1],color='blue',alpha=0.3)\n",
    "\n",
    "        ax[0][it].plot(ll_tnp_cen_nnlo[0],ll_tnp_cen_nnlo[1],\n",
    "                       color='red',label='NNLO')\n",
    "        ax[0][it].errorbar(error_x_val,var_band_cen_NNLO,error_cen_NNLO,color='Red',linestyle='none')\n",
    "\n",
    "        ax[0][it].fill_between(ll_tnp_cen_nnlo[0],\n",
    "                               ll_tnp_min_nnlo[1],\n",
    "                               ll_tnp_max_nnlo[1],color='red',alpha=0.4)\n",
    "\n",
    "\n",
    "        ax[0][it].text(0.02,0.9,'Errorbands: TNP (bands)',transform=ax[0][it].transAxes)\n",
    "        ax[0][it].grid(True)\n",
    "\n",
    "        if it == 0: ax[0][it].legend(loc='lower left',ncol=3)\n",
    "\n",
    "        y_range_min = np.amin([ll_var_cen_lo[1],\n",
    "                               ll_tnp_min_nlo[1],ll_tnp_min_nnlo[1],\n",
    "                               ll_var_min_nlo[1],ll_var_min_nnlo[1]\n",
    "                               ])\n",
    "        y_range_max = np.amax([ll_var_cen_lo[1],\n",
    "                               ll_tnp_max_nlo[1],ll_tnp_max_nnlo[1],\n",
    "                               ll_var_max_nlo[1],ll_var_max_nnlo[1]\n",
    "                               ])\n",
    "        if y_range_min < y_range_min_common: y_range_min_common = y_range_min\n",
    "        if y_range_max > y_range_max_common: y_range_max_common = y_range_max\n",
    "\n",
    "        x_range_min = np.amin(ll_var_cen_lo[0])\n",
    "        x_range_max = np.amax(ll_var_cen_lo[0])\n",
    "        \n",
    "\n",
    "        ax[0][it].set_xlim(x_range_min,x_range_max)\n",
    "\n",
    "        # TNP with quad method\n",
    "        ax[1][it].plot(ll_var_cen_lo[0],ll_var_cen_lo[1],color='limegreen',label='LO')\n",
    "\n",
    "        ax[1][it].plot(ll_tnp_cen_nlo[0],ll_tnp_cen_nlo[1],\n",
    "                           color='blue',label='NLO')\n",
    "        ax[1][it].errorbar(error_x_val,var_band_cen_NLO,error_cen_NLO,color='blue',linestyle='none')\n",
    "\n",
    "        ax[1][it].fill_between(ll_tnp_cen_quad_nlo[0],\n",
    "                               ll_tnp_min_quad_nlo[1],\n",
    "                               ll_tnp_max_quad_nlo[1],color='blue',alpha=0.3)\n",
    "\n",
    "        ax[1][it].plot(ll_tnp_cen_quad_nnlo[0],ll_tnp_cen_quad_nnlo[1],\n",
    "                       color='red',label='NNLO')\n",
    "        ax[1][it].errorbar(error_x_val,var_band_cen_NNLO,error_cen_NNLO,color='Red',linestyle='none')\n",
    "\n",
    "        ax[1][it].fill_between(ll_tnp_cen_quad_nnlo[0],\n",
    "                               ll_tnp_min_quad_nnlo[1],\n",
    "                               ll_tnp_max_quad_nnlo[1],color='red',alpha=0.4)\n",
    "\n",
    "        ax[1][it].text(0.02,0.9,'Errorbands: TNP (quad.)',transform=ax[1][it].transAxes)\n",
    "        ax[1][it].grid(True)        \n",
    "        ax[1][it].set_xlim(x_range_min,x_range_max)\n",
    "\n",
    "\n",
    "        # scale variations\n",
    "        ax[2][it].plot(ll_var_cen_lo[0],ll_var_cen_lo[1],color='limegreen',label='LO')\n",
    "        ax[2][it].fill_between(ll_var_cen_lo[0],\n",
    "                               ll_var_min_lo[1],\n",
    "                               ll_var_max_lo[1],color='limegreen',alpha=0.3)\n",
    "\n",
    "        ax[2][it].plot(ll_var_cen_nlo[0],ll_var_cen_nlo[1],color='blue',label='NLO')\n",
    "        ax[2][it].errorbar(error_x_val,var_band_cen_NLO,error_cen_NLO,color='blue',linestyle='none')\n",
    "        ax[2][it].fill_between(ll_var_cen_nlo[0],\n",
    "                               ll_var_min_nlo[1],\n",
    "                               ll_var_max_nlo[1],color='blue',alpha=0.3)\n",
    "\n",
    "        ax[2][it].plot(ll_var_cen_nnlo[0],ll_var_cen_nnlo[1],color='red',label='NNLO')\n",
    "        ax[2][it].fill_between(ll_var_cen_nnlo[0],\n",
    "                               ll_var_min_nnlo[1],\n",
    "                               ll_var_max_nnlo[1],color='red',alpha=0.4)\n",
    "        ax[2][it].errorbar(error_x_val,var_band_cen_NNLO,error_cen_NNLO,color='Red',linestyle='none')\n",
    "\n",
    "        ax[2][it].text(0.02,0.9,'Errorbands: 3-Point scale var.',transform=ax[2][it].transAxes)\n",
    "        ax[2][it].grid(True)\n",
    "        ax[2][it].set_xlim(x_range_min,x_range_max)\n",
    "\n",
    "\n",
    "        ax[2][it].set_xlabel(processes[proc][\"observables\"][ob]['label'])\n",
    "    for it in range(0,len(processes[proc][\"observables\"])):\n",
    "      ax[0][it].set_ylim(y_range_min_common*0.8,y_range_max_common*1.2)\n",
    "      ax[1][it].set_ylim(y_range_min_common*0.8,y_range_max_common*1.2)\n",
    "      ax[2][it].set_ylim(y_range_min_common*0.8,y_range_max_common*1.2)\n",
    "    pp.savefig();\n",
    "    plt.show()\n",
    "    pp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kR_kyctM3dKE"
   },
   "source": [
    "## Plots for alternative parameterisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XvnOzoglQWNO",
    "outputId": "1fca863a-7bf7-423f-f345-b9c749a90fd4"
   },
   "outputs": [],
   "source": [
    "for proc in processes:\n",
    "  for scale in processes[proc][\"scales\"]:\n",
    "\n",
    "    pp = PdfPages('./pdfs/hightea_'+proc+\"_\"+scale+'-alt.pdf')\n",
    "\n",
    "    n_obs = len(processes[proc][\"observables\"])\n",
    "\n",
    "    if n_obs == 1:\n",
    "        fig, ax = plt.subplots(3, 2, figsize=(10./3.*2,6.), constrained_layout=True)#\n",
    "    else:\n",
    "        fig, ax = plt.subplots(3, n_obs, figsize=(10./3.*n_obs,6.), constrained_layout=True)#\n",
    "\n",
    "    fig.suptitle(processes[proc]['label']+r' central scale: $\\mu = $ '+processes[proc]['scales'][scale]['label']+\n",
    "                 \" Alternative parameterisation\")\n",
    "    y_range_min_common = 10.\n",
    "    y_range_max_common = 0.\n",
    "    for it, ob in enumerate(processes[proc][\"observables\"]):\n",
    "\n",
    "        # data preparation\n",
    "\n",
    "        norm = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].values[:,0]\n",
    "\n",
    "        # statistical uncertainties of central prediction\n",
    "        error_x_val = np.array(processes[proc][\"observables\"][ob]['binning'])\n",
    "        error_x_val = (error_x_val[:-1]+error_x_val[1:])/2.\n",
    "        error_cen_NLO  = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].errors[:,0]/norm\n",
    "        error_cen_NNLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"].errors[:,0]/norm\n",
    "\n",
    "        ll_error_cen_nlo  = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],error_cen_NLO)\n",
    "        ll_error_cen_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],error_cen_NNLO)\n",
    "        \n",
    "        # tnp errors band\n",
    "        tnp_band_cen_NLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].values[:,0]/norm\n",
    "        tnp_band_max_NLO = processes[proc][\"observables\"][ob][\"tnp_alt_\"+scale+\"_NLO_max\"]/norm\n",
    "        tnp_band_min_NLO = processes[proc][\"observables\"][ob][\"tnp_alt_\"+scale+\"_NLO_min\"]/norm\n",
    "\n",
    "        tnp_band_cen_NNLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"].values[:,0]/norm\n",
    "        tnp_band_max_NNLO = processes[proc][\"observables\"][ob][\"tnp_alt_\"+scale+\"_NNLO_max\"]/norm\n",
    "        tnp_band_min_NNLO = processes[proc][\"observables\"][ob][\"tnp_alt_\"+scale+\"_NNLO_min\"]/norm\n",
    "\n",
    "        ll_tnp_cen_nlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_band_cen_NLO)\n",
    "        ll_tnp_max_nlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_band_max_NLO)\n",
    "        ll_tnp_min_nlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_band_min_NLO)\n",
    "\n",
    "        ll_tnp_cen_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_band_cen_NNLO)\n",
    "        ll_tnp_max_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_band_max_NNLO)\n",
    "        ll_tnp_min_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_band_min_NNLO)\n",
    "\n",
    "        # tnp quad\n",
    "        tnp_quad_cen_NLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].values[:,0]/norm\n",
    "        tnp_quad_max_NLO = processes[proc][\"observables\"][ob][\"tnp_alt_\"+scale+\"_NLO_quad_max\"]/norm\n",
    "        tnp_quad_min_NLO = processes[proc][\"observables\"][ob][\"tnp_alt_\"+scale+\"_NLO_quad_min\"]/norm\n",
    "\n",
    "        tnp_quad_cen_NNLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"].values[:,0]/norm\n",
    "        tnp_quad_max_NNLO = processes[proc][\"observables\"][ob][\"tnp_alt_\"+scale+\"_NNLO_quad_max\"]/norm\n",
    "        tnp_quad_min_NNLO = processes[proc][\"observables\"][ob][\"tnp_alt_\"+scale+\"_NNLO_quad_min\"]/norm\n",
    "\n",
    "        ll_tnp_cen_quad_nlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_quad_cen_NLO)\n",
    "        ll_tnp_max_quad_nlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_quad_max_NLO)\n",
    "        ll_tnp_min_quad_nlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_quad_min_NLO)\n",
    "\n",
    "        ll_tnp_cen_quad_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_quad_cen_NNLO)\n",
    "        ll_tnp_max_quad_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_quad_max_NNLO)\n",
    "        ll_tnp_min_quad_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],tnp_quad_min_NNLO)\n",
    "        \n",
    "        # scale uncertainties\n",
    "        var_band_cen_LO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_LO\"].values[:,0]/norm\n",
    "        var_band_max_LO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_LO\"].values[:,2]/norm\n",
    "        var_band_min_LO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_LO\"].values[:,1]/norm\n",
    "\n",
    "        var_band_cen_NLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].values[:,0]/norm\n",
    "        var_band_max_NLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].values[:,2]/norm\n",
    "        var_band_min_NLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].values[:,1]/norm\n",
    "\n",
    "        var_band_cen_NNLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"].values[:,0]/norm\n",
    "        var_band_max_NNLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"].values[:,2]/norm\n",
    "        var_band_min_NNLO = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"].values[:,1]/norm\n",
    "\n",
    "        ll_var_cen_lo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],var_band_cen_LO)\n",
    "        ll_var_max_lo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],var_band_max_LO)\n",
    "        ll_var_min_lo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],var_band_min_LO)\n",
    "\n",
    "        ll_var_cen_nlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],var_band_cen_NLO)\n",
    "        ll_var_max_nlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],var_band_max_NLO)\n",
    "        ll_var_min_nlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],var_band_min_NLO)\n",
    "\n",
    "        ll_var_cen_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],var_band_cen_NNLO)\n",
    "        ll_var_max_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],var_band_max_NNLO)\n",
    "        ll_var_min_nnlo = get_ll_from_data(processes[proc][\"observables\"][ob]['binning'],var_band_min_NNLO)\n",
    "\n",
    "\n",
    "        # general layout\n",
    "        if it == 0:\n",
    "            ax[1][it].set_ylabel('ratio to NLO')\n",
    "            ax[0][it].tick_params(labelbottom=False,bottom=False)\n",
    "            ax[1][it].tick_params(labelbottom=False,bottom=False)\n",
    "        else:\n",
    "            ax[0][it].tick_params(labelbottom = False, bottom = False, left = False, labelleft = False)\n",
    "            ax[1][it].tick_params(labelbottom = False, bottom = False, left = False, labelleft = False)\n",
    "            ax[2][it].tick_params(left = False, labelleft = False)\n",
    "\n",
    "        # TNP uncertainty with band method\n",
    "        ax[0][it].plot(ll_var_cen_lo[0],ll_var_cen_lo[1],color='limegreen',label='LO')\n",
    "\n",
    "        ax[0][it].plot(ll_tnp_cen_nlo[0],ll_tnp_cen_nlo[1],\n",
    "                           color='blue',label='NLO')\n",
    "        ax[0][it].errorbar(error_x_val,var_band_cen_NLO,error_cen_NLO,color='blue',linestyle='none')\n",
    "\n",
    "        ax[0][it].fill_between(ll_tnp_cen_nlo[0],\n",
    "                               ll_tnp_min_nlo[1],\n",
    "                               ll_tnp_max_nlo[1],color='blue',alpha=0.3)\n",
    "\n",
    "        ax[0][it].plot(ll_tnp_cen_nnlo[0],ll_tnp_cen_nnlo[1],\n",
    "                       color='red',label='NNLO')\n",
    "        ax[0][it].errorbar(error_x_val,var_band_cen_NNLO,error_cen_NNLO,color='Red',linestyle='none')\n",
    "\n",
    "        ax[0][it].fill_between(ll_tnp_cen_nnlo[0],\n",
    "                               ll_tnp_min_nnlo[1],\n",
    "                               ll_tnp_max_nnlo[1],color='red',alpha=0.4)\n",
    "\n",
    "\n",
    "        ax[0][it].text(0.02,0.9,'Errorbands: TNP (bands)',transform=ax[0][it].transAxes)\n",
    "        ax[0][it].grid(True)\n",
    "\n",
    "        if it == 0: ax[0][it].legend(loc='lower left',ncol=3)\n",
    "\n",
    "        y_range_min = np.amin([ll_var_cen_lo[1],\n",
    "                               ll_tnp_min_nlo[1],ll_tnp_min_nnlo[1],\n",
    "                               ll_var_min_nlo[1],ll_var_min_nnlo[1]\n",
    "                               ])\n",
    "        y_range_max = np.amax([ll_var_cen_lo[1],\n",
    "                               ll_tnp_max_nlo[1],ll_tnp_max_nnlo[1],\n",
    "                               ll_var_max_nlo[1],ll_var_max_nnlo[1]\n",
    "                               ])\n",
    "        if y_range_min < y_range_min_common: y_range_min_common = y_range_min\n",
    "        if y_range_max > y_range_max_common: y_range_max_common = y_range_max\n",
    "\n",
    "        x_range_min = np.amin(ll_var_cen_lo[0])\n",
    "        x_range_max = np.amax(ll_var_cen_lo[0])\n",
    "        \n",
    "\n",
    "        ax[0][it].set_xlim(x_range_min,x_range_max)\n",
    "\n",
    "        # TNP with quad method\n",
    "        ax[1][it].plot(ll_var_cen_lo[0],ll_var_cen_lo[1],color='limegreen',label='LO')\n",
    "\n",
    "        ax[1][it].plot(ll_tnp_cen_nlo[0],ll_tnp_cen_nlo[1],\n",
    "                           color='blue',label='NLO')\n",
    "        ax[1][it].errorbar(error_x_val,var_band_cen_NLO,error_cen_NLO,color='blue',linestyle='none')\n",
    "\n",
    "        ax[1][it].fill_between(ll_tnp_cen_quad_nlo[0],\n",
    "                               ll_tnp_min_quad_nlo[1],\n",
    "                               ll_tnp_max_quad_nlo[1],color='blue',alpha=0.3)\n",
    "\n",
    "        ax[1][it].plot(ll_tnp_cen_quad_nnlo[0],ll_tnp_cen_quad_nnlo[1],\n",
    "                       color='red',label='NNLO')\n",
    "        ax[1][it].errorbar(error_x_val,var_band_cen_NNLO,error_cen_NNLO,color='Red',linestyle='none')\n",
    "\n",
    "        ax[1][it].fill_between(ll_tnp_cen_quad_nnlo[0],\n",
    "                               ll_tnp_min_quad_nnlo[1],\n",
    "                               ll_tnp_max_quad_nnlo[1],color='red',alpha=0.4)\n",
    "\n",
    "        ax[1][it].text(0.02,0.9,'Errorbands: TNP (quad.)',transform=ax[1][it].transAxes)\n",
    "        ax[1][it].grid(True)        \n",
    "        ax[1][it].set_xlim(x_range_min,x_range_max)\n",
    "\n",
    "\n",
    "        # scale variations\n",
    "        ax[2][it].plot(ll_var_cen_lo[0],ll_var_cen_lo[1],color='limegreen',label='LO')\n",
    "        ax[2][it].fill_between(ll_var_cen_lo[0],\n",
    "                               ll_var_min_lo[1],\n",
    "                               ll_var_max_lo[1],color='limegreen',alpha=0.3)\n",
    "\n",
    "        ax[2][it].plot(ll_var_cen_nlo[0],ll_var_cen_nlo[1],color='blue',label='NLO')\n",
    "        ax[2][it].errorbar(error_x_val,var_band_cen_NLO,error_cen_NLO,color='blue',linestyle='none')\n",
    "        ax[2][it].fill_between(ll_var_cen_nlo[0],\n",
    "                               ll_var_min_nlo[1],\n",
    "                               ll_var_max_nlo[1],color='blue',alpha=0.3)\n",
    "\n",
    "        ax[2][it].plot(ll_var_cen_nnlo[0],ll_var_cen_nnlo[1],color='red',label='NNLO')\n",
    "        ax[2][it].fill_between(ll_var_cen_nnlo[0],\n",
    "                               ll_var_min_nnlo[1],\n",
    "                               ll_var_max_nnlo[1],color='red',alpha=0.4)\n",
    "        ax[2][it].errorbar(error_x_val,var_band_cen_NNLO,error_cen_NNLO,color='Red',linestyle='none')\n",
    "\n",
    "        ax[2][it].text(0.02,0.9,'Errorbands: 3-Point scale var.',transform=ax[2][it].transAxes)\n",
    "        ax[2][it].grid(True)\n",
    "        ax[2][it].set_xlim(x_range_min,x_range_max)\n",
    "\n",
    "\n",
    "        ax[2][it].set_xlabel(processes[proc][\"observables\"][ob]['label'])\n",
    "    for it in range(0,len(processes[proc][\"observables\"])):\n",
    "      ax[0][it].set_ylim(y_range_min_common*0.8,y_range_max_common*1.2)\n",
    "      ax[1][it].set_ylim(y_range_min_common*0.8,y_range_max_common*1.2)\n",
    "      ax[2][it].set_ylim(y_range_min_common*0.8,y_range_max_common*1.2)\n",
    "    pp.savefig();\n",
    "    plt.show()\n",
    "    pp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PQLce2IP35M"
   },
   "source": [
    "# Fit of TNP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzbfbaBhu5aD"
   },
   "source": [
    "## Visualisation of polynomial parameterizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 843
    },
    "id": "Ah7MimJTu4QQ",
    "outputId": "0942d5fb-a36f-4563-d684-6c529bf19106"
   },
   "outputs": [],
   "source": [
    "plt.title('Sampled Bernstein polynomials')\n",
    "\n",
    "n_par = 3\n",
    "for it in range(0,100):\n",
    "  c = [rn.uniform(-1,1) for xx in range(0,n_par)]\n",
    "  xx= np.linspace(0,1.,100)\n",
    "  plt.plot(xx,f_x(c,xx))\n",
    "plt.show()\n",
    "\n",
    "plt.title('Sampled Chebyshev polynomials')\n",
    "for it in range(0,100):\n",
    "  c = [rn.uniform(-1,1) for xx in range(0,n_par)]\n",
    "  xx= np.linspace(-1,1.,100)\n",
    "  plt.plot(xx,g_x(c,xx))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6tjD40USeGx"
   },
   "source": [
    "## Distribution of TNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "from scipy.interpolate import BPoly\n",
    "from sklearn.linear_model import Ridge\n",
    "from scipy.stats import binom\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "def bernstein(x,deg):\n",
    "  return binom.pmf(np.arange(1+deg),deg,x.reshape(-1,1))\n",
    "\n",
    "def f_x(x, theta1, theta2, theta3):\n",
    "    theta = [theta1, theta2, theta3]\n",
    "    bp = BPoly([ [xx] for xx in theta], [0.,1.])\n",
    "    return bp(x)\n",
    "\n",
    "def g_x(x, theta1, theta2, theta3):\n",
    "    return 0.5*(theta1+theta2*x+theta3*(2.*x**2-1))\n",
    "\n",
    "thetas_bernstein = []\n",
    "thetas_chebyshev = []\n",
    "\n",
    "for proc in ['pp_evmv_13000', 'pp_ww_13000', 'pp_aa_8000', 'pp_tt_13000_172.5']:\n",
    "  for scale in processes[proc][\"scales\"]:\n",
    "    for it, ob in enumerate(processes[proc][\"observables\"]):\n",
    "        for func in [f_x, g_x]:\n",
    "            \n",
    "          central_lo = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_LO\"].values[:,0]\n",
    "          central_nlo = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].values[:,0]\n",
    "          central_nnlo = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"].values[:,0]\n",
    "\n",
    "          dsig2dsig0 = np.divide(processes[proc][\"observables\"][ob][\"sigmab2_\"+scale],processes[proc][\"observables\"][ob][\"sigmab0_\"+scale])\n",
    "          dsig1dsig0 = np.divide(processes[proc][\"observables\"][ob][\"sigmab1_\"+scale],processes[proc][\"observables\"][ob][\"sigmab0_\"+scale])\n",
    "\n",
    "          dsig2dsig0_fracsqerr = processes[proc][\"observables\"][ob][\"sigmab2_fracsqerr_\"+scale] + processes[proc][\"observables\"][ob][\"sigmab0_fracsqerr_\"+scale]\n",
    "          dsig1dsig0_fracsqerr = processes[proc][\"observables\"][ob][\"sigmab1_fracsqerr_\"+scale] + processes[proc][\"observables\"][ob][\"sigmab0_fracsqerr_\"+scale]\n",
    "\n",
    "          bins=processes[proc][\"observables\"][ob]['binning']\n",
    "          if func == f_x:\n",
    "              rescale_bins = (np.array(bins)-bins[0])/(bins[-1]-bins[0])\n",
    "              plt_xs = np.linspace(0, 1, 1000)\n",
    "          elif func == g_x:\n",
    "              rescale_bins = (np.array(bins)-(bins[0]+bins[-1])/2.)/(bins[-1]-bins[0])*2.\n",
    "              plt_xs = np.linspace(-1, 1, 1000)\n",
    "          new_bins = np.array( [ (rescale_bins[i+1]+rescale_bins[i])/2. for i in range(rescale_bins.size-1)])\n",
    "          bin_widths = np.array( [ (rescale_bins[i+1]-rescale_bins[i]) for i in range(rescale_bins.size-1)])\n",
    "\n",
    "          ave = sum([ bin_widths[i]*dsig1dsig0[i] for i in range(dsig1dsig0.size)])\n",
    "          polymodel = np.divide(dsig2dsig0,dsig1dsig0)\n",
    "          polymodel_err = np.multiply(polymodel, np.sqrt(dsig2dsig0_fracsqerr+dsig1dsig0_fracsqerr))\n",
    "          model_params, model_cov = curve_fit(func, new_bins, polymodel, sigma=polymodel_err)\n",
    "          if func == f_x:\n",
    "              thetas_bernstein.append(model_params)\n",
    "          else:\n",
    "              thetas_chebyshev.append(model_params)\n",
    "\n",
    "theta1s_b = np.array(thetas_bernstein)[:,0]\n",
    "theta2s_b = np.array(thetas_bernstein)[:,1]\n",
    "theta3s_b = np.array(thetas_bernstein)[:,2]\n",
    "theta1s_c = np.array(thetas_chebyshev)[:,0]\n",
    "theta2s_c = np.array(thetas_chebyshev)[:,1]\n",
    "theta3s_c = np.array(thetas_chebyshev)[:,2]\n",
    "\n",
    "print('Results of statistical tests with Bernstein polynomials')\n",
    "print(stats.shapiro(theta1s_b))\n",
    "print(stats.shapiro(theta2s_b))\n",
    "print(stats.shapiro(theta3s_b))\n",
    "print(stats.anderson(theta1s_b))\n",
    "print(stats.anderson(theta2s_b))\n",
    "print(stats.anderson(theta3s_b))\n",
    "\n",
    "print('Results of statistical tests with Chebyshev polynomials')\n",
    "print(stats.shapiro(theta1s_c))\n",
    "print(stats.shapiro(theta2s_c))\n",
    "print(stats.shapiro(theta3s_c))\n",
    "print(stats.anderson(theta1s_c))\n",
    "print(stats.anderson(theta2s_c))\n",
    "print(stats.anderson(theta3s_c))\n",
    "\n",
    "pp_b = PdfPages('./pdfs/hightea_thetas_bernstein.pdf')\n",
    "pp_c = PdfPages('./pdfs/hightea_thetas_chebyshev.pdf')\n",
    "\n",
    "n_obs = len(thetas_bernstein[0])\n",
    "\n",
    "fig, ax = plt.subplots(1, n_obs, figsize=(10./3.*n_obs,4.), constrained_layout=True)\n",
    "\n",
    "fig.suptitle(r'TNPs in Bernstein parameterisation')\n",
    "\n",
    "ax[0].hist(theta1s_b,bins=np.linspace(-1.,2.,9), density = True, color='xkcd:sky blue', edgecolor='xkcd:dark sky blue')\n",
    "ax[1].hist(theta2s_b,bins=np.linspace(-2.,4.,9), density = True, color='xkcd:sky blue', edgecolor='xkcd:dark sky blue')\n",
    "ax[2].hist(theta3s_b,bins=np.linspace(-2.,2.,8), density = True, color='xkcd:sky blue', edgecolor='xkcd:dark sky blue')\n",
    "ax[0].set_xlabel(r'$\\theta_1$')\n",
    "ax[1].set_xlabel(r'$\\theta_2$')\n",
    "ax[2].set_xlabel(r'$\\theta_3$')\n",
    "ax[0].text(0.1,0.9,r'$\\mu=$'+str(\"%.2f\" %round(np.mean(theta1s_b),2)),transform=ax[0].transAxes)\n",
    "ax[0].text(0.1,0.85,r'$\\sigma=$'+str(\"%.2f\" %round(np.std(theta1s_b),2)),transform=ax[0].transAxes)\n",
    "ax[1].text(0.1,0.9,r'$\\mu=$'+str(\"%.2f\" %round(np.mean(theta2s_b),2)),transform=ax[1].transAxes)\n",
    "ax[1].text(0.1,0.85,r'$\\sigma=$'+str(\"%.2f\" %round(np.std(theta2s_b),2)),transform=ax[1].transAxes)\n",
    "ax[2].text(0.1,0.9,r'$\\mu=$'+str(\"%.2f\" %round(np.mean(theta3s_b),2)),transform=ax[2].transAxes)\n",
    "ax[2].text(0.1,0.85,r'$\\sigma=$'+str(\"%.2f\" %round(np.std(theta3s_b),2)),transform=ax[2].transAxes)\n",
    "\n",
    "ax[0].plot(np.arange(-1, 2, 0.01), norm.pdf(np.arange(-1, 2, 0.01), np.mean(theta1s_b), np.std(theta1s_b)), color='xkcd:orange')\n",
    "ax[1].plot(np.arange(-2, 4, 0.01), norm.pdf(np.arange(-2, 4, 0.01), np.mean(theta2s_b), np.std(theta2s_b)), color='xkcd:orange')\n",
    "ax[2].plot(np.arange(-2, 2, 0.01), norm.pdf(np.arange(-2, 2, 0.01), np.mean(theta3s_b), np.std(theta3s_b)), color='xkcd:orange')\n",
    "\n",
    "pp_b.savefig()\n",
    "plt.show()\n",
    "pp_b.close()\n",
    "\n",
    "ax[0].clear()\n",
    "ax[1].clear()\n",
    "ax[2].clear()\n",
    "\n",
    "fig, ax = plt.subplots(1, n_obs, figsize=(10./3.*n_obs,4.), constrained_layout=True)\n",
    "\n",
    "fig.suptitle(r'TNPs in Chebyshev parameterisation')\n",
    "\n",
    "ax[0].hist(theta1s_c,bins=np.linspace(-2.,4.,9), density = True, color='xkcd:sea green', edgecolor='xkcd:dark sea green')\n",
    "ax[1].hist(theta2s_c,bins=np.linspace(-2.,2.,9), density = True, color='xkcd:sea green', edgecolor='xkcd:dark sea green')\n",
    "ax[2].hist(theta3s_c,bins=np.linspace(-2.,2.,9), density = True, color='xkcd:sea green', edgecolor='xkcd:dark sea green')\n",
    "ax[0].set_xlabel(r'$\\theta_1$')\n",
    "ax[1].set_xlabel(r'$\\theta_2$')\n",
    "ax[2].set_xlabel(r'$\\theta_3$')\n",
    "ax[0].text(0.1,0.9,r'$\\mu=$'+str(\"%.2f\" %round(np.mean(theta1s_c),2)),transform=ax[0].transAxes)\n",
    "ax[0].text(0.1,0.85,r'$\\sigma=$'+str(\"%.2f\" %round(np.std(theta1s_c),2)),transform=ax[0].transAxes)\n",
    "ax[1].text(0.1,0.9,r'$\\mu=$'+str(\"%.2f\" %round(np.mean(theta2s_c),2)),transform=ax[1].transAxes)\n",
    "ax[1].text(0.1,0.85,r'$\\sigma=$'+str(\"%.2f\" %round(np.std(theta2s_c),2)),transform=ax[1].transAxes)\n",
    "ax[2].text(0.1,0.9,r'$\\mu=$'+str(\"%.2f\" %round(np.mean(theta3s_c),2)),transform=ax[2].transAxes)\n",
    "ax[2].text(0.1,0.85,r'$\\sigma=$'+str(\"%.2f\" %round(np.std(theta3s_c),2)),transform=ax[2].transAxes)\n",
    "\n",
    "ax[0].plot(np.arange(-2, 4, 0.01), norm.pdf(np.arange(-2, 4, 0.01), np.mean(theta1s_c), np.std(theta1s_c)), color='xkcd:orange')\n",
    "ax[1].plot(np.arange(-2, 2, 0.01), norm.pdf(np.arange(-2, 2, 0.01), np.mean(theta2s_c), np.std(theta2s_c)), color='xkcd:orange')\n",
    "ax[2].plot(np.arange(-2, 2, 0.01), norm.pdf(np.arange(-2, 2, 0.01), np.mean(theta3s_c), np.std(theta3s_c)), color='xkcd:orange')\n",
    "\n",
    "pp_c.savefig()\n",
    "plt.show()\n",
    "\n",
    "pp_c.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "from scipy.interpolate import BPoly\n",
    "from sklearn.linear_model import Ridge\n",
    "from scipy.stats import binom\n",
    "import scipy.stats as stats\n",
    "\n",
    "def bernstein(x,deg):\n",
    "  return binom.pmf(np.arange(1+deg),deg,x.reshape(-1,1))\n",
    "\n",
    "def f_x(x, theta1, theta2, theta3):\n",
    "    theta = [theta1, theta2, theta3]\n",
    "    bp = BPoly([ [xx] for xx in theta], [0.,1.])\n",
    "    return bp(x)\n",
    "\n",
    "def g_x(x, theta1, theta2, theta3):\n",
    "    return 0.5*(theta1+theta2*x+theta3*(2.*x**2-1))\n",
    "\n",
    "polyorder = 2\n",
    "polytype = \"C\" # \"C\" for Chebyshev and \"B\" for Bernstein\n",
    "\n",
    "thetas = []\n",
    "\n",
    "for proc in processes:\n",
    "  for scale in processes[proc][\"scales\"]:\n",
    "    for it, ob in enumerate(processes[proc][\"observables\"]):\n",
    "\n",
    "      central_lo = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_LO\"].values[:,0]\n",
    "      central_nlo = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NLO\"].values[:,0]\n",
    "      central_nnlo = processes[proc][\"observables\"][ob][\"res_\"+scale+\"_NNLO\"].values[:,0]\n",
    "\n",
    "      dsig2dsig0 = np.divide(processes[proc][\"observables\"][ob][\"sigmab2_\"+scale],processes[proc][\"observables\"][ob][\"sigmab0_\"+scale])\n",
    "      dsig1dsig0 = np.divide(processes[proc][\"observables\"][ob][\"sigmab1_\"+scale],processes[proc][\"observables\"][ob][\"sigmab0_\"+scale])\n",
    "\n",
    "      dsig2dsig0_fracsqerr = processes[proc][\"observables\"][ob][\"sigmab2_fracsqerr_\"+scale] + processes[proc][\"observables\"][ob][\"sigmab0_fracsqerr_\"+scale]\n",
    "      dsig1dsig0_fracsqerr = processes[proc][\"observables\"][ob][\"sigmab1_fracsqerr_\"+scale] + processes[proc][\"observables\"][ob][\"sigmab0_fracsqerr_\"+scale]\n",
    "   \n",
    "      bins=processes[proc][\"observables\"][ob]['binning']\n",
    "      if polytype == \"B\":\n",
    "          rescale_bins = (np.array(bins)-bins[0])/(bins[-1]-bins[0])\n",
    "      elif polytype == \"C\":\n",
    "          rescale_bins = (np.array(bins)-(bins[0]+bins[-1])/2.)/(bins[-1]-bins[0])*2.\n",
    "      new_bins = np.array( [ (rescale_bins[i+1]+rescale_bins[i])/2. for i in range(rescale_bins.size-1)])\n",
    "      bin_widths = np.array( [ (rescale_bins[i+1]-rescale_bins[i]) for i in range(rescale_bins.size-1)])\n",
    "\n",
    "      ave = sum([ bin_widths[i]*dsig1dsig0[i] for i in range(dsig1dsig0.size)])\n",
    "      polymodel = np.divide(dsig2dsig0,dsig1dsig0)\n",
    "      polymodel_err = np.multiply(polymodel, np.sqrt(dsig2dsig0_fracsqerr+dsig1dsig0_fracsqerr))\n",
    "\n",
    "      if polytype == \"B\":\n",
    "          plt_xs = np.linspace(0, 1, 1000)\n",
    "          model_params, model_cov = curve_fit(f_x, new_bins, polymodel, sigma=polymodel_err)\n",
    "      elif polytype == \"C\":\n",
    "          plt_xs = np.linspace(-1, 1, 1000)\n",
    "          model_params, model_cov = curve_fit(g_x, new_bins, polymodel, sigma=polymodel_err)\n",
    "        \n",
    "      plt.scatter(new_bins.ravel(), polymodel.ravel())\n",
    "      plt.errorbar(new_bins.ravel(), polymodel.ravel(), yerr=np.abs(polymodel_err), fmt='o')\n",
    "      if polytype == \"B\":\n",
    "          plt.plot(plt_xs, f_x(plt_xs,*model_params), 'r')\n",
    "      elif polytype == \"C\":\n",
    "          plt.plot(plt_xs, g_x(plt_xs,*model_params), 'r') \n",
    "      print(proc, ob, scale)\n",
    "      print(model_params)\n",
    "      plt.show()\n",
    "      \n",
    "      thetas.append(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MVPvFD6vv-g6",
    "TeHHolFzvstM",
    "RyAZku9v7KsZ",
    "ZSBvFRHNwjBp"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
